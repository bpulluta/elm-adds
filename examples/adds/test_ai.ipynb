{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openai.embeddings_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_embedding, cosine_similarity\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtiktoken\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'openai.embeddings_utils'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set environment variables (replace with your actual values)\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"YOUR_ENDPOINT\"\n",
    "\n",
    "API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\") \n",
    "RESOURCE_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\") \n",
    "\n",
    "# Configure OpenAI client\n",
    "import openai\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_key = API_KEY\n",
    "openai.api_base = RESOURCE_ENDPOINT\n",
    "openai.api_version = \"2022-12-01\"\n",
    "\n",
    "# Check deployed models\n",
    "url = openai.api_base + \"/openai/deployments?api-version=2022-12-01\" \n",
    "r = requests.get(url, headers={\"api-key\": API_KEY})\n",
    "print(r.text)\n",
    "\n",
    "# Download and load the dataset\n",
    "!curl \"https://raw.githubusercontent.com/Azure-Samples/Azure-OpenAI-Docs-Samples/main/Samples/Tutorials/Embeddings/data/bill_sum_data.csv\" --output bill_sum_data.csv\n",
    "df = pd.read_csv('bill_sum_data.csv')\n",
    "df_bills = df[['text', 'summary', 'title']]\n",
    "\n",
    "# Data cleaning function\n",
    "def normalize_text(s):\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    s = re.sub(r\". ,\",\"\",s)\n",
    "    s = s.replace(\"..\",\".\").replace(\". .\",\".\").replace(\"\\n\", \"\").strip()\n",
    "    return s\n",
    "\n",
    "df_bills['text'] = df_bills[\"text\"].apply(normalize_text)\n",
    "\n",
    "# Tokenization and filtering based on token limit\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "df_bills['n_tokens'] = df_bills[\"text\"].apply(lambda x: len(tokenizer.encode(x)))\n",
    "df_bills = df_bills[df_bills.n_tokens < 8192]\n",
    "\n",
    "# Generate embeddings\n",
    "def generate_embeddings(text, model=\"text-embedding-ada-002\"):\n",
    "    return openai.Embedding.create(input=[text], model=model).data[0].embedding\n",
    "\n",
    "df_bills['ada_v2'] = df_bills[\"text\"].apply(generate_embeddings)\n",
    "\n",
    "# Search function\n",
    "def search_docs(df, user_query, top_n=4):\n",
    "    embedding = generate_embeddings(user_query)\n",
    "    df[\"similarities\"] = df.ada_v2.apply(lambda x: cosine_similarity(x, embedding))\n",
    "    res = df.sort_values(\"similarities\", ascending=False).head(top_n)\n",
    "    return res\n",
    "\n",
    "# Example search query\n",
    "res = search_docs(df_bills, \"Can I get information on cable company tax revenue?\")\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"720df3d64998425ab3a454902c77d9b1\"\n",
    "\n",
    "# Step 2: Initialize the OpenAI model\n",
    "openai_model = OpenAI(model=\"text-embedding-ada-002\")\n",
    "\n",
    "document_loader = DirectoryLoader(\"/Users/bpulluta/elm/examples/adds/pdfs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for OpenAIEmbeddings\nmodel\n  str type expected (type=type_error.str)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Step 4: Create embeddings for your documents\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAIEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopenai_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/elm/.env/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:183\u001b[0m, in \u001b[0;36mwarn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mtype\u001b[39m):\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _obj_type:\n\u001b[0;32m--> 183\u001b[0m         _obj_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    184\u001b[0m     wrapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    185\u001b[0m     _name \u001b[38;5;241m=\u001b[39m _name \u001b[38;5;129;01mor\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\n",
      "File \u001b[0;32m~/elm/.env/lib/python3.9/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for OpenAIEmbeddings\nmodel\n  str type expected (type=type_error.str)"
     ]
    }
   ],
   "source": [
    "# Step 4: Create embeddings for your documents\n",
    "embeddings = OpenAIEmbeddings(model=openai_model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Load documents and create FAISS vector store\n",
    "documents = document_loader.load()\n",
    "vector_store = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "# Step 6: Set up the prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"documents\", \"question\"],\n",
    "    template=\"Answer the question based on the following documents: {documents}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 7: Create the QA system\n",
    "qa_system = VectorDBQA(\n",
    "    vector_store=vector_store,\n",
    "    llm=openai_model,\n",
    "    prompt_template=prompt_template\n",
    ")\n",
    "\n",
    "# Step 8: Define a function to query the chatbot\n",
    "def query_chatbot(question):\n",
    "    response = qa_system.ask(question)\n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "question = \"What is the capital of France?\"\n",
    "answer = query_chatbot(question)\n",
    "print(answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
